{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning (& Reinforcement Learning)\n",
    "\n",
    "In this section we will work exclusively with local models. We will first see how a small local model perform on the task of coverage check out of the box. We will then see how we can improve that model further with Supervised Fine Tuning. Finally we will see if reinforcement learning can help by adding reasoning capabilities to our model specifically tailored to reason about claims.\n",
    "\n",
    "For illustration purposes but also to fit in limited memory constraint environment of this workshop, we will use [Qwen2.5 3B](https://qwenlm.github.io/blog/qwen2.5/) which really shows impressive performances for it's size. (careful the 3B version is non-commercialy licensed)\n",
    "\n",
    "To actually perform the finetuning we will leverage the library [unsloth](https://docs.unsloth.ai/). Unsloth makes it possible to finetune LLMs extremly fast and with very limited resources by using a few tricks:\n",
    "- it uses [QLoRA](https://arxiv.org/abs/2305.14314) by default and enabling to load model in 4bit (we will talk about it a bit more)\n",
    "- custom [OpenAI's triton](https://openai.com/index/triton/) CUDA kernels to speed up finetuning\n",
    "- it recently also optimized inference further by leveraging [vllm](https://github.com/vllm-project/vllm) on optimized inference server.\n",
    "\n",
    "Finally, still using unsloth we will be able to use Grouped Policy Optimization (GRPO) the same technique DeepSeek R1 is using to transform our model into a reasoning model tailored to reason about claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5 baseline\n",
    "\n",
    "Before even starting to finetune our performing reinforcement learning, we need to establish our baseline with Qwen 2.5 3B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and prepare the dataset\n",
    "\n",
    "We will load the same dataset as used in the previous part of this workshop. That is 400 synthtetic claims about a car insurance policy of AXA UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset with 400 claims.\n"
     ]
    }
   ],
   "source": [
    "# we use pydantic models to help you navigate / type the dataset\n",
    "from models import ClaimsDataset\n",
    "\n",
    "with open(\"data/claims_dataset_v2_manual.json\", \"r\") as f:\n",
    "    dataset = ClaimsDataset.model_validate_json(f.read())\n",
    "\n",
    "print(f\"loaded dataset with {len(dataset.root)} claims.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Claim in module models:\n",
      "\n",
      "class Claim(pydantic.main.BaseModel)\n",
      " |  Claim(*, description: str, explanation: str, coverage: bool, sources: list[models.Source], source_rule: models.SourceRuleContainer, limit_unit: str | None = None, limit_amount: float | None = None, limit_targets: list[str]) -> None\n",
      " |\n",
      " |  Represents an individual insurance claim with all its details and\n",
      " |  associated rules.\n",
      " |\n",
      " |  Attributes:\n",
      " |      description (str): Detailed description of the claim scenario\n",
      " |      explanation (str): Additional explanation or notes about the claim\n",
      " |      coverage (bool): Whether the claim is covered under the policy\n",
      " |      sources (list[Source]): Relevant policy document excerpts supporting\n",
      " |          the claim\n",
      " |      source_rule (SourceRuleContainer): Collection of rules applicable to\n",
      " |          the claim\n",
      " |      limit_unit (Optional[str]): Currency unit for the claim limit (e.g.,\n",
      " |          \"GBP\")\n",
      " |      limit_amount (Optional[float]): Maximum amount covered for the claim\n",
      " |      limit_targets (list[str]): list of entities to whom the limit applies\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Claim\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'coverage': <class 'bool'>, 'description': <class '...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'models.Sou...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'coverage': FieldInfo(annotation=bool, required...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Claim\", validator=Mode...\n",
      " |\n",
      " |  __signature__ = <Signature (*, description: str, explanation: st...| N...\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_computed_fields\n",
      " |      Get metadata about the computed fields defined on the model.\n",
      " |\n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |\n",
      " |      Returns:\n",
      " |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields\n",
      " |      Get metadata about the fields defined on the model.\n",
      " |\n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |\n",
      " |      Returns:\n",
      " |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can use python 'help' to see the content of the pydantic model\n",
    "from models import Claim\n",
    "\n",
    "help(Claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 126 covered claims and 274 not covered claims.\n"
     ]
    }
   ],
   "source": [
    "claims = dataset.root\n",
    "covered_claims = [claim for claim in claims if claim.coverage]\n",
    "not_covered_claims = [claim for claim in claims if not claim.coverage]\n",
    "\n",
    "print(f\"there are {len(covered_claims)} covered claims and {len(not_covered_claims)} not covered claims.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the 400 claims it's fairly unbalanced with 68.5% of claims not covered and 31.5% covered. So when splitting into train / test set we will need to check the proportion in the split to make sure we are not too far off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split train / test dataset\n",
    "\n",
    "To make sure our results are comparable we first split the dataset into a training and testing set. We will establish our baseline only on the test set. The train set will be use to finetune the model and then the finetuned model will be evaluated on the test set again. \n",
    "\n",
    "Note here that in a real world setting I would probably set a Stratified K fold CV to ensure the proportion of covered / not covered across several splits. For the purpose of this workshop we keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 400 claims into 320 training claims and 80\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# shuffle randomly the claims with reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(claims)\n",
    "\n",
    "# keep 80% as training set, 20% as testing set.\n",
    "split_ratio = 0.8\n",
    "train_size = int(len(claims) * split_ratio)\n",
    "\n",
    "train_claims = claims[:train_size]\n",
    "test_claims = claims[train_size:]\n",
    "\n",
    "print(f\"split {len(claims)} claims into {len(train_claims)} training claims and {len(test_claims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0625% covered, 70.9375% not covered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "covered_train_claims = [claim for claim in train_claims if claim.coverage]\n",
    "not_covered_train_claims = [claim for claim in train_claims if not claim.coverage]\n",
    "\n",
    "print(f\"{len(covered_train_claims)*100/len(train_claims)}% covered, {len(not_covered_train_claims)*100/len(train_claims)}% not covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.25% covered, 58.75% not covered\n"
     ]
    }
   ],
   "source": [
    "covered_test_claims = [claim for claim in test_claims if claim.coverage]\n",
    "not_covered_test_claims = [claim for claim in test_claims if not claim.coverage]\n",
    "\n",
    "print(f\"{len(covered_test_claims)*100/len(test_claims)}% covered, {len(not_covered_test_claims)*100/len(test_claims)}% not covered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slightly difference in the proportion of covered not covered between our training and testing set that could potentially impact our end results. To do it better we could make a stratified split for example using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "def split_train_test(\n",
    "    dataset: ClaimsDataset, split_ratio: float = 0.8\n",
    ") -> tuple[list[Claim], list[Claim]]:\n",
    "    \"\"\"\n",
    "    Split the dataset into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to split.\n",
    "        split_ratio: The ratio of the dataset to use for the train set.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists of claims: the train set and the test set.\n",
    "    \"\"\"\n",
    "    # shuffle the dataset\n",
    "    # we could use a stratified split to make sure we have\n",
    "    # the same proportion of covered and not covered claims in the train and test set\n",
    "    dataset_copy = deepcopy(dataset)\n",
    "\n",
    "    # shuffle the dataset to take a random split\n",
    "    random.shuffle(dataset_copy.root)\n",
    "\n",
    "    train_size = int(len(dataset_copy.root) * split_ratio)\n",
    "\n",
    "    return dataset_copy.root[:train_size], dataset_copy.root[train_size:]\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = split_train_test(dataset)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 0.35)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len([claim for claim in test_dataset if claim.coverage]),\n",
    "    len([claim for claim in test_dataset if claim.coverage]) / len(test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 0.65)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len([claim for claim in test_dataset if not claim.coverage]),\n",
    "    len([claim for claim in test_dataset if not claim.coverage]) / len(test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish baseline performance Qwen2.5(3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.4\n",
      "CUDA available: True\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Get the current device info if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-10 14:13:21 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 43.61%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 7.61 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.9 GB. Also swap space = 5 GB.\n",
      "WARNING 02-10 14:13:29 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-10 14:13:36 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-10 14:13:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "INFO 02-10 14:13:37 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-10 14:13:37 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-10 14:13:37 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 02-10 14:13:37 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-10 14:13:38 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446a5f79f11543ed8fb53e78209131ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877baedf4f194df78d63e52d3d12d579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-10 14:13:40 model_runner.py:1115] Loading model weights took 2.2265 GB\n",
      "INFO 02-10 14:13:40 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-10 14:13:42 worker.py:267] Memory profiling takes 1.22 seconds\n",
      "INFO 02-10 14:13:42 worker.py:267] the current vLLM instance can use total_gpu_memory (7.61GiB) x gpu_memory_utilization (0.44) = 3.32GiB\n",
      "INFO 02-10 14:13:42 worker.py:267] model weights take 2.23GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 0.27GiB.\n",
      "INFO 02-10 14:13:42 executor_base.py:110] # CUDA blocks: 490, # CPU blocks: 9102\n",
      "INFO 02-10 14:13:42 executor_base.py:115] Maximum concurrency for 1024 tokens per request: 7.66x\n",
      "INFO 02-10 14:13:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:12<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-10 14:14:01 model_runner.py:1562] Graph capturing finished in 13 secs, took 0.43 GiB\n",
      "INFO 02-10 14:14:01 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 20.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.5 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "max_seq_length = 1024  # Can increase for longer reasoning traces\n",
    "lora_rank = 64  # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=True,  # Enable vLLM fast inference\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.5,  # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are an Insurance Claim Expert.\n",
    "You are given a claim description and a list of sources extracted from the insurance policy.\n",
    "You need to determine if the claim is covered by the insurance policy based on the sources.\n",
    "\n",
    "Claim description:\n",
    "{claim.description}\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Format:\n",
    "Return only \"covered\" or \"not covered\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You are an Insurance Claim Expert.\n",
      "You are given a claim description and a list of sources extracted from the insurance policy.\n",
      "You need to determine if the claim is covered by the insurance policy based on the sources.\n",
      "\n",
      "Claim description:\n",
      "While driving through a busy intersection, another vehicle ran a red light and collided with the side of my car. The impact damaged the front bumper and shattered the left headlight. The repair involved replacing the bumper and headlight with newer models that enhanced the car's appearance and functionality beyond its original condition. Could the improvements made during the repair affect the coverage decision?\n",
      "\n",
      "Sources:\n",
      "1. You are not covered for the following:\n",
      "2. Loss of or damage to your car arising from or as a result of water freezing in the cooling circulation system of your car.\n",
      "\n",
      "Format:\n",
      "Return only \"covered\" or \"not covered\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def claim_to_prompt(claim: Claim):\n",
    "    \"\"\"apply chat template and format the prompt\"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(\n",
    "                    claim=claim,\n",
    "                    sources=\"\\n\".join(\n",
    "                        [\n",
    "                            f\"{i + 1}. {source.paragraph}\"\n",
    "                            for i, source in enumerate(claim.sources)\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(claim_to_prompt(test_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You are an Insurance Claim Expert.\n",
      "You are given a claim description and a list of sources extracted from the insurance policy.\n",
      "You need to determine if the claim is covered by the insurance policy.\n",
      "\n",
      "Claim description:\n",
      "I discovered someone had attempted to steal my car. The driver's side door lock was damaged, and the dashboard was dismantled, with the stereo missing. Is there any provision for covering transportation and accomodation?\n",
      "\n",
      "Sources:\n",
      "1. If your car, accessories or spare parts are lost, stolen or damaged, we will: - repair the damage; - replace what is lost or damaged and is too expensive to repair; or - pay you the cost of the loss or damage.\n",
      "2. If your car is damaged, we will use one of our recommended repairers to repair it. If you choose not to use them, we may not pay more than our recommended repairer would have charged and we may choose to settle the claim by a financial payment. Following damage to your car, we may move your car to a place of safe and free storage pending settlement of any claim.\n",
      "3. Where your car is not recovered following a theft or is beyond economical repair we will pay you the market value of your car, including accessories and spare parts at the time they are lost, stolen or damaged.\n",
      "4. If we settle a claim as a total loss, we will then take ownership of your car.\n",
      "5. Accessories and spare parts of your car, which are in your private garage at the time of their loss or damage, will also be covered.\n",
      "6. You are not covered for the following:\n",
      "7. Loss or theft of your car by deception. This includes, but is not limited to: Loss or theft as a result of handing the keys of your car over to someone who claims to be a buyer or agent without taking precautions to ensure your car is returned to you. An example of an acceptable precaution is to attend the test drive with the prospective buyer. Loss or theft as a result of someone purchasing your car using a payment method which does not result in you receiving the payment for your car.\n",
      "8. Loss or damage to your car by theft or attempted theft if you or anyone else has left it unlocked or with keys or keyless entry system in your car, or on it.\n",
      "\n",
      "Format:\n",
      "Return only \"covered\" or \"not covered\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [claim_to_prompt(claim) for claim in test_dataset]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens covered: 1\n",
      "num tokens not covered: 2\n"
     ]
    }
   ],
   "source": [
    "# to set our sampling parameters its important we check how much token is \"covered\" and \"not covered\"\n",
    "# tokenizer usually use bpe bit piece encoding algorithm to create tokens on a dataset\n",
    "# see this great video from Andrej Karpathy to understand how bpe works:\n",
    "\n",
    "print(f\"num tokens covered: {len(tokenizer.tokenize('covered'))}\")\n",
    "print(f\"num tokens not covered: {len(tokenizer.tokenize('not covered'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:07<00:00, 10.51it/s, est. speed input: 3812.68 toks/s, output: 21.02 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2,  # set to 2 as not covered is 2 tokens\n",
    ")\n",
    "outputs = model.fast_generate(\n",
    "    prompts,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = []\n",
    "for response_output in outputs:\n",
    "    responses.append(response_output.outputs[0].text)\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 14 claims as covered\n",
      "Predicted 66 claims as not covered\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Predicted {len([response for response in responses if response == 'covered'])} claims as covered\"\n",
    ")\n",
    "print(\n",
    "    f\"Predicted {len([response for response in responses if response == 'not covered'])} claims as not covered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.725\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.35714285714285715\n",
      "F1 Score: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "y_pred = [response.strip() == \"covered\" for response in responses]\n",
    "y_true = [claim.coverage for claim in test_dataset]\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_true, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert responses to boolean (True for 'covered', False for 'not covered')\n",
    "y_pred = [response.strip() == \"covered\" for response in responses]\n",
    "y_true = [claim.coverage for claim in test_dataset]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Not Covered\", \"Covered\"],\n",
    "    yticklabels=[\"Not Covered\", \"Covered\"],\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do Supervised Fine Tuning but we traditional Superverised Fine Tuning by default\n",
    "learn from each token similarly to what is done during pre-training. For most use cases\n",
    "this is not very useful (you are learning from the prompt and the output although it might bring\n",
    "the model more into the topic as well) at the cost of extra resources:\n",
    "\n",
    "Actually it can be interesting to try both approach, in reality if you use some API\n",
    "you won't be able to try both approach and it is very likely that you will be using \n",
    "next token prediction with the loss computed and gradient average on the whole text instead\n",
    "of just the result.\n",
    "\n",
    "- https://github.com/huggingface/trl/issues/632 trl issue where this is explained and there is no definite conclusion on the matter\n",
    "- https://huggingface.co/docs/trl/en/sft_trainer#train-on-completions-only we will train on completion only to compare with custom head also lower resource need\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You are an Insurance Claim Expert.\n",
      "You are given a claim description and a list of sources extracted from the insurance policy.\n",
      "You need to determine if the claim is covered by the insurance policy.\n",
      "\n",
      "Claim description:\n",
      "I discovered someone had attempted to steal my car. The driver's side door lock was damaged, and the dashboard was dismantled, with the stereo missing. Is there any provision for covering transportation and accomodation?\n",
      "\n",
      "Sources:\n",
      "1. If your car, accessories or spare parts are lost, stolen or damaged, we will: - repair the damage; - replace what is lost or damaged and is too expensive to repair; or - pay you the cost of the loss or damage.\n",
      "2. If your car is damaged, we will use one of our recommended repairers to repair it. If you choose not to use them, we may not pay more than our recommended repairer would have charged and we may choose to settle the claim by a financial payment. Following damage to your car, we may move your car to a place of safe and free storage pending settlement of any claim.\n",
      "3. Where your car is not recovered following a theft or is beyond economical repair we will pay you the market value of your car, including accessories and spare parts at the time they are lost, stolen or damaged.\n",
      "4. If we settle a claim as a total loss, we will then take ownership of your car.\n",
      "5. Accessories and spare parts of your car, which are in your private garage at the time of their loss or damage, will also be covered.\n",
      "6. You are not covered for the following:\n",
      "7. Loss or theft of your car by deception. This includes, but is not limited to: Loss or theft as a result of handing the keys of your car over to someone who claims to be a buyer or agent without taking precautions to ensure your car is returned to you. An example of an acceptable precaution is to attend the test drive with the prospective buyer. Loss or theft as a result of someone purchasing your car using a payment method which does not result in you receiving the payment for your car.\n",
      "8. Loss or damage to your car by theft or attempted theft if you or anyone else has left it unlocked or with keys or keyless entry system in your car, or on it.\n",
      "\n",
      "Format:\n",
      "Return only \"covered\" or \"not covered\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'claim_to_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we create a huggingface dataset which makes it easy to load into a pytorch dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and to use with the trl library / SFTTrainer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we just need a text column for SFT (next token prediction)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(\n\u001b[1;32m      8\u001b[0m     [\n\u001b[1;32m      9\u001b[0m         {\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m# we just add covered / not covered to the prompt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m# depending on the claim coverage\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mclaim_to_prompt\u001b[49m(claim)\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovered\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m claim\u001b[38;5;241m.\u001b[39mcoverage \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot covered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m         }\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m claim \u001b[38;5;129;01min\u001b[39;00m train_dataset\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'claim_to_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "# we create a huggingface dataset which makes it easy to load into a pytorch dataset\n",
    "# and to use with the trl library / SFTTrainer\n",
    "\n",
    "# we just need a text column for SFT (next token prediction)\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(\n",
    "    [\n",
    "        {\n",
    "            # we just add covered / not covered to the prompt\n",
    "            # depending on the claim coverage\n",
    "            \"text\": claim_to_prompt(claim)\n",
    "            + (\"covered\" if claim.coverage else \"not covered\")\n",
    "        }\n",
    "        for claim in train_dataset\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You are an Insurance Claim Expert.\n",
      "You are given a claim description and a list of sources extracted from the insurance policy.\n",
      "You need to determine if the claim is covered by the insurance policy.\n",
      "\n",
      "Claim description:\n",
      "During a routine inspection, government officials arrived at the garage and declared that several car spare parts, including the catalytic converter and the exhaust system, were not compliant with new environmental regulations. Without prior notice, they ordered the immediate confiscation of these parts, citing the need to uphold public safety standards. Is there any recourse available for unexpected regulatory compliance issues?\n",
      "\n",
      "Sources:\n",
      "1. You are not covered for the following:\n",
      "2. Confiscation or requisition or destruction by, or under the order of, any government or public or land authority.\n",
      "\n",
      "Format:\n",
      "Return only \"covered\" or \"not covered\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "not covered\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator is responsible to format the text into batch of inputs for the model\n",
    "# default collator would just TODO(Guillaume): add a way to show what's the difference\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0c6c50d9734795ae273466188969b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # batch size = 2\n",
    "        gradient_accumulation_steps=4,  # accumulate gradients 4 times -> equivalent to batch size 8\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,  # Set this for 1 full training run.\n",
    "        # max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 320 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 40\n",
      " \"-____-\"     Number of trainable parameters = 119,734,272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 02:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:08<00:00,  9.51it/s, est. speed input: 3449.95 toks/s, output: 19.02 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2,  # set to 2 as not covered is 2 tokens\n",
    ")\n",
    "outputs = model.fast_generate(\n",
    "    prompts,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not covered',\n",
       " 'covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered',\n",
       " 'not covered']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = []\n",
    "for response_output in outputs:\n",
    "    responses.append(response_output.outputs[0].text)\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 16 claims as covered\n",
      "Predicted 64 claims as not covered\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Predicted {len([response for response in responses if response == 'covered'])} claims as covered\"\n",
    ")\n",
    "print(\n",
    "    f\"Predicted {len([response for response in responses if response == 'not covered'])} claims as not covered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.75\n",
      "Recall: 0.42857142857142855\n",
      "F1 Score: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "y_pred = [response.strip() == \"covered\" for response in responses]\n",
    "y_true = [claim.coverage for claim in test_dataset]\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know observe better results, we can tweak more to get even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert responses to boolean (True for 'covered', False for 'not covered')\n",
    "y_pred = [response.strip() == \"covered\" for response in responses]\n",
    "y_true = [claim.coverage for claim in test_dataset]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Not Covered\", \"Covered\"],\n",
    "    yticklabels=[\"Not Covered\", \"Covered\"],\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing regular next token prediction as it is often the case we can also just modify the lm head and do classification instead. After all we don't need to predict over 151k values...\n",
    "\n",
    "code found: https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-10 17:50:30 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2070. Max memory: 7.607 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# from peft import LoftQConfig\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=False,  # Disable vLLM fast inference not compatible with training classification head\n",
    "    max_lora_rank=64,\n",
    "    gpu_memory_utilization=0.5,  # Reduce if out of memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2048])\n",
      "torch.Size([151936, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "yes_token_id = tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "no_token_id = tokenizer.encode(\"no\", add_special_tokens=False)[0]\n",
    "\n",
    "# keep only the yes and no tokens from lm_head\n",
    "par = torch.nn.Parameter(\n",
    "    torch.vstack(\n",
    "        [model.lm_head.weight[no_token_id, :], model.lm_head.weight[yes_token_id, :]]\n",
    "    )\n",
    ")\n",
    "print(par.shape)\n",
    "print(model.lm_head.weight.shape)\n",
    "model.lm_head.weight = par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings().modules_to_save.default.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/Projects/AMLD2025/.venv/lib/python3.12/site-packages/unsloth/models/_utils.py:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n",
      "Unsloth 2025.2.5 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n",
      "trainable parameters: 29937664\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        # now we can fine tune the lm_head\n",
    "        \"lm_head\",  # can easily be trained because it has only 2 tokens\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=True,  # We support rank stabilized LoRA\n",
    ")\n",
    "print(\n",
    "    \"trainable parameters:\",\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this custom collator is needed to change the sequence labels from yes_token_id and no_token_id to 1 and 0. It also trains only on the last token of the sequence.\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def torch_call(self, examples) -> dict:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # Find the last non-padding token\n",
    "            last_token_idx = (\n",
    "                (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            )\n",
    "            # Set all labels to ignore_index except for the last token\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            # The old labels for the Yes and No tokens need to be mapped to 1 and 0\n",
    "            batch[\"labels\"][i, last_token_idx] = (\n",
    "                1 if batch[\"labels\"][i, last_token_idx] == yes_token_id else 0\n",
    "            )\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO\n",
    "\n",
    "With GRPO we can train our model to reason about the claim and the corresponding contract sources before giving us an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning enable the model to improve itself over the time leveraging a reward model. Traditionally people where using PPO and derivatives to perform RL(HF) to improve human preference on LLM. More recently deepseek introduced GRPO (Grouped Policy Optimization) which simplify PPO removing the need for a dedicated value model computing the advantage on each token prediction and relying on averaging the result from multiple output of the same policy model instead.This is what is behind the R1 model and it enables many model to become \"reasoning\" model.\n",
    "\n",
    "[PPO vs GRPO](./images/PPO_VS_GRPO.png)\n",
    "\n",
    "https://arxiv.org/pdf/2402.03300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRPO is quite interesting as it allows us to define reward function giving a certain reward to a given response for from the \"Policy Model\" KL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default reward functions from: https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_claim_dataset_for_grpo() -> Dataset:\n",
    "    data = Dataset.from_list(\n",
    "        [\n",
    "            {\n",
    "                \"question\": claim_to_prompt(claim),\n",
    "                \"answer\": {'covered' if claim.coverage else 'not covered'},\n",
    "            }\n",
    "            for claim in train_dataset\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    data = data.map(\n",
    "        lambda x: {  # type: ignore\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "            ],\n",
    "            \"answer\": x[\"answer\"],\n",
    "        }\n",
    "    )  # type: ignore\n",
    "    return data  # type: ignore\n",
    "\n",
    "\n",
    "dataset = get_claim_dataset_for_grpo()\n",
    "\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    q = prompts[0][-1][\"content\"]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(\n",
    "        \"-\" * 20,\n",
    "        f\"Question:\\n{q}\",\n",
    "        f\"\\nAnswer:\\n{answer[0]}\",\n",
    "        f\"\\nResponse:\\n{responses[0]}\",\n",
    "        f\"\\nExtracted:\\n{extracted_responses[0]}\",\n",
    "    )\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
    "    return count\n",
    "\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm=True,  # use vLLM for fast inference!\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # Increase to 4 for smoother training\n",
    "    num_generations=8,  # Decrease if out of memory\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps=250,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"outputs\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
