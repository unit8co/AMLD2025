{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMbXQWjCNnUD"
      },
      "source": [
        "# Graph-based RAG System Workshop\n",
        "\n",
        "## What is Graph RAG?\n",
        "\n",
        "Graph RAG (Retrieval Augmented Generation) is an advanced approach to information retrieval and generation that combines the power of graph databases with Large Language Models (LLMs). Unlike traditional RAG systems that rely solely on vector similarity, Graph RAG leverages both semantic relationships and structural connections between pieces of information.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Vector Search**: Uses embeddings to find semantically similar content\n",
        "   - Converts queries into vector representations\n",
        "   - Finds similar nodes based on semantic similarity\n",
        "   - Applies importance scores and type-specific adjustments\n",
        "\n",
        "2. **Graph Structure**: Maintains relationships between information\n",
        "   - Stores different types of entities (e.g., BENEFIT, EXCLUSION, LIMIT)\n",
        "   - Captures relationships (e.g., APPLIES_TO, CONTAINS, EXCLUDES)\n",
        "   - Enables traversal through connected information\n",
        "\n",
        "3. **Context Enrichment**: Enhances retrieval with graph relationships\n",
        "   - Extracts subgraphs around relevant nodes\n",
        "   - Considers relationship types and hop depth\n",
        "   - Provides richer context for LLM analysis\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "- **Better Context**: Captures relationships between pieces of information\n",
        "- **Structured Reasoning**: Follows explicit paths and relationships\n",
        "- **Enhanced Accuracy**: Combines semantic and structural relevance\n",
        "- **Explainable Results**: Clear path of reasoning through the graph\n",
        "\n",
        "In this workshop, we'll implement a Graph RAG system for insurance policy analysis, demonstrating how to combine Memgraph (drop in replacement for Neo4j) graph database, OpenAI embeddings, and LLM-powered analysis for intelligent document processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph Databases vs Relational Databases\n",
        "\n",
        "### What is a Graph Database?\n",
        "\n",
        "A graph database is a database that uses graph structures to store and represent data, with:\n",
        "- **Nodes**: Representing entities (like people, products, or policies)\n",
        "- **Edges**: Representing relationships between entities\n",
        "- **Properties**: Attributes that can be attached to both nodes and edges\n",
        "\n",
        "### Key Differences from Relational Databases\n",
        "\n",
        "1. **Data Model**\n",
        "   - **Relational**: Data is stored in tables with rows and columns\n",
        "   - **Graph**: Data is stored as nodes and relationships, mirroring real-world connections\n",
        "\n",
        "2. **Relationships**\n",
        "   - **Relational**: Relationships are implied through foreign keys and joins\n",
        "   - **Graph**: Relationships are first-class citizens, explicitly stored and traversed\n",
        "\n",
        "3. **Query Performance**\n",
        "   - **Relational**: Joins become expensive with increasing data complexity\n",
        "   - **Graph**: Relationship traversal remains constant regardless of total database size\n",
        "\n",
        "### Advantages of Graph Databases\n",
        "\n",
        "1. **Natural Data Modeling**\n",
        "   - Intuitive representation of connected data\n",
        "   - Easier to model complex relationships\n",
        "   - More flexible schema evolution\n",
        "\n",
        "2. **Performance for Connected Data**\n",
        "   - Faster traversal of relationships\n",
        "   - No need for expensive JOIN operations\n",
        "   - Constant-time traversal regardless of database size\n",
        "\n",
        "3. **Query Capabilities**\n",
        "   - Native support for pattern matching\n",
        "   - Efficient path finding and graph algorithms\n",
        "   - Better support for recursive queries\n",
        "\n",
        "4. **Use Case Advantages**\n",
        "   - Recommendation engines\n",
        "   - Fraud detection\n",
        "   - Social networks\n",
        "   - Knowledge graphs\n",
        "   - Supply chain management\n",
        "\n",
        "### Example: Insurance Policy Analysis\n",
        "\n",
        "In our case, graph databases are particularly useful because insurance policies have:\n",
        "- Complex relationships between clauses\n",
        "- Hierarchical structures (policies → sections → clauses)\n",
        "- Multiple interconnected entities (coverages, exclusions, conditions)\n",
        "- Need for path-based queries (\"What exclusions apply to this coverage?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System Architecture\n",
        "```mermaid\n",
        "graph TD\n",
        "subgraph User_Interface\n",
        "Q[User Query/Claim]\n",
        "end\n",
        "subgraph Vector_Search\n",
        "E[OpenAI Embeddings]\n",
        "VS[Vector Similarity Search]\n",
        "end\n",
        "subgraph Graph_DB[Memgraph Database]\n",
        "N[Nodes<br/>- Benefits<br/>- Exclusions<br/>- Limits<br/>- Clauses]\n",
        "R[Relationships<br/>- Applies To<br/>- Contains<br/>- Excludes]\n",
        "VI[Vector Index]\n",
        "end\n",
        "subgraph Context_Builder\n",
        "SG[Subgraph Extraction]\n",
        "CE[Context Enrichment]\n",
        "end\n",
        "subgraph LLM_Analysis\n",
        "P[Policy Analysis]\n",
        "D[Decision Making]\n",
        "end\n",
        "subgraph Output\n",
        "RES[Response<br/>- Coverage Decision<br/>- Explanation<br/>- Applicable Limits<br/>- Relevant Exclusions]\n",
        "end\n",
        "Q --> E\n",
        "E --> VS\n",
        "VS --> VI\n",
        "VI --> N\n",
        "N <--> R\n",
        "N --> SG\n",
        "R --> SG\n",
        "SG --> CE\n",
        "CE --> P\n",
        "P --> D\n",
        "D --> RES\n",
        "style Graph_DB fill:#f9f,stroke:#333,stroke-width:2px\n",
        "style Vector_Search fill:#bbf,stroke:#333,stroke-width:2px\n",
        "style Context_Builder fill:#bfb,stroke:#333,stroke-width:2px\n",
        "style LLM_Analysis fill:#ffb,stroke:#333,stroke-width:2px\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flow Explanation:\n",
        "\n",
        "1. **Query Processing**:\n",
        "   - User submits an insurance claim query\n",
        "   - Query is converted to vector embeddings using OpenAI\n",
        "\n",
        "2. **Retrieval**:\n",
        "   - Vector similarity search finds relevant policy nodes\n",
        "   - Graph database provides structural context\n",
        "   - Subgraph extraction captures related information\n",
        "\n",
        "3. **Analysis**:\n",
        "   - Context builder combines vector and graph information\n",
        "   - LLM analyzes claim against policy context\n",
        "   - Generates structured response with coverage decision\n",
        "\n",
        "4. **Response**:\n",
        "   - Returns coverage decision\n",
        "   - Provides explanation with policy references\n",
        "   - Lists applicable limits and exclusions"
      ]
    },
    {
<<<<<<< HEAD
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph Generation\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "    A[Read Markdown File] --> B[Create Hierarchical Chunks]\n",
        "    B --> C[Process Each Chunk]\n",
        "    \n",
        "    subgraph \"Chunk Processing\"\n",
        "        C --> D[GPT Knowledge Extraction]\n",
        "        D --> E1[Create Nodes]\n",
        "        D --> E2[Create Relationships]\n",
        "        \n",
        "        E1 --> F1[Generate Embeddings]\n",
        "        E1 --> F2[Store Node Properties]\n",
        "        \n",
        "        F1 --> G[Vector Index]\n",
        "        F2 --> H[Node Storage]\n",
        "        \n",
        "        E2 --> I[Relationship Storage]\n",
        "    end\n",
        "    \n",
        "    subgraph \"Post-Processing\"\n",
        "        J[Add Similarity Relationships]\n",
        "        K[Cross-chunk Analysis]\n",
        "        L[Global Properties]\n",
        "    end\n",
        "    \n",
        "    C --> J\n",
        "    J --> K\n",
        "    K --> L\n",
        "    \n",
        "    subgraph \"Node Types\"\n",
        "        N1[LIMIT]\n",
        "        N2[CONCEPT]\n",
        "        N3[CLAUSE]\n",
        "        N4[DEFINITION]\n",
        "    end\n",
        "    \n",
        "    subgraph \"Relationship Types\"\n",
        "        R1[SEMANTICALLY_SIMILAR]\n",
        "        R2[RELATED_LIMIT]\n",
        "        R3[HAS_SUBCLAUSE]\n",
        "        R4[RELATED_CONCEPT]\n",
        "        R5[Custom Relations from GPT]\n",
        "    end\n",
        "    \n",
        "    subgraph \"Global Properties\"\n",
        "        P1[Betweenness Centrality]\n",
        "        P2[Community Detection]\n",
        "    end\n",
        "    \n",
        "    classDef blue fill:#2374f7,color:#fff,stroke:#fff\n",
        "    classDef green fill:#48a868,color:#fff,stroke:#fff\n",
        "    classDef orange fill:#eb7434,color:#fff,stroke:#fff\n",
        "    \n",
        "    class A,B,C blue\n",
        "    class N1,N2,N3,N4 green\n",
        "    class R1,R2,R3,R4,R5 orange\n",
        "```"
      ]
    },
    {
=======
>>>>>>> 933cb9995bd7f152e2044d306d16ca908bf88ed9
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z90-EbLANnUE",
        "outputId": "1c5f0b5a-a3da-47be-b2d9-68de6c351e62"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "x9S2oVo7NnUF"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra-RTYSWNnUF",
        "outputId": "3cce2429-7e8f-474c-8fd5-8290b248b02d"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Configuration Section\n",
        "# =====================================\n",
        "# Replace these with your actual credentials\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "MEMGRAPH_HOST = \"bolt+ssc://3.123.228.201:7687\"\n",
        "MEMGRAPH_USER = userdata.get('MEMGRAPH_USER')\n",
        "MEMGRAPH_PASSWORD = userdata.get('MEMGRAPH_PASSWORD')\n",
        "\n",
        "print(MEMGRAPH_USER, MEMGRAPH_PASSWORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kN-pYg9SgBds"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration settings for the retriever component.\n",
        "This file contains all parameters that influence the retrieval process.\n",
        "\"\"\"\n",
        "\n",
        "# Vector Search Settings\n",
        "VECTOR_SIMILARITY_THRESHOLD = 0.40  # Minimum similarity score for vector search results\n",
        "MIN_IMPORTANCE_SCORE = 0.0  # Minimum importance score for nodes to be considered\n",
        "MAX_RESULTS = 7  # Maximum number of similar nodes to retrieve\n",
        "\n",
        "# Graph Traversal Settings\n",
        "MAX_HOP_DEPTH = 5  # Maximum number of hops when extracting subgraph relationships\n",
        "IGNORE_RELATIONSHIP_TYPES = [\"SEMANTICALLY_SIMILAR\"]  # Relationship types to ignore during traversal\n",
        "\n",
        "# Type-Specific Similarity Adjustments\n",
        "SIMILARITY_ADJUSTMENTS = {\n",
        "    \"EXCLUSION\": 0.8,  # Reduce importance of exclusions by 20%\n",
        "    \"LIMIT\": 1.2,  # Boost importance of limits by 20%\n",
        "}\n",
        "\n",
        "# LLM Analysis Settings\n",
        "SYSTEM_PROMPT_TEMPERATURE = 0.0  # Temperature for initial claim analysis\n",
        "MAX_RETRIES = 3  # Maximum number of retries for LLM calls\n",
        "RETRY_TEMPERATURE_INCREMENT = 0.1  # Temperature increment on each retry\n",
        "\n",
        "# Cache Settings\n",
        "ENABLE_CLAIM_GRAPH_CACHE = True  # Whether to cache extracted claim graphs\n",
        "ENABLE_EMBEDDING_CACHE = True  # Whether to cache claim embeddings\n",
        "\n",
        "# Logging Settings\n",
        "LOG_SIMILAR_NODES = True  # Whether to log found similar nodes\n",
        "LOG_SUBGRAPH_EXTRACTION = True  # Whether to log subgraph extraction details\n",
        "DEBUG_FIRST_N_RESULTS = 3  # Number of top results to log in debug mode\n",
        "\n",
        "VECTOR_INDEX_NAME = \"vector_index\"\n",
        "\n",
        "\n",
        "# Entity and Relation Types\n",
        "POSSIBLE_ENTITIES = [\n",
        "    \"BENEFIT\",\n",
        "    \"CLAIM_PROCEDURE\",\n",
        "    \"CLAUSE\",\n",
        "    \"CONDITION\",\n",
        "    \"COVERAGE\",\n",
        "    \"DEDUCTIBLE\",\n",
        "    \"DEFINITION\",\n",
        "    \"ENDORSEMENT\",\n",
        "    \"EVENT\",\n",
        "    \"EXCESS\",\n",
        "    \"EXCLUSION\",\n",
        "    \"LIMIT\",\n",
        "    \"ORGANIZATION\",\n",
        "    \"PAYOUT\",\n",
        "    \"PERIL\",\n",
        "    \"PERSON\",\n",
        "    \"POLICYHOLDER\",\n",
        "    \"PREMIUM\",\n",
        "    \"RISK_OBJECT\",\n",
        "    \"SCHEDULE\",\n",
        "    \"SECTION\",\n",
        "    \"SERVICE\",\n",
        "    \"SUBSECTION\",\n",
        "    \"TERM\",\n",
        "]\n",
        "\n",
        "POSSIBLE_RELATIONSHIPS = [\n",
        "    \"AMENDS\",\n",
        "    \"APPLIES_TO\",\n",
        "    \"CONTAINS\",\n",
        "    \"COVERS\",\n",
        "    \"DESCRIBES\",\n",
        "    \"EXCLUDES\",\n",
        "    \"FOLLOWS\",\n",
        "    \"HAS_CERTIFICATE\",\n",
        "    \"HAS_SCHEDULE\",\n",
        "    \"IMPACTS\",\n",
        "    \"LEADS_TO\",\n",
        "    \"MENTIONS\",\n",
        "    \"PAYABLE_FOR\",\n",
        "    \"PRECEDES\",\n",
        "    \"PROVIDES\",\n",
        "    \"REFERENCES\",\n",
        "    \"VALID_DURING\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "EoBqBu9oOniE"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Memgraph Client\n",
        "# =====================================\n",
        "class MemgraphClient:\n",
        "    \"\"\"A client for interacting with Memgraph database.\n",
        "\n",
        "    This client provides comprehensive functionality for:\n",
        "    - Managing graph database connections\n",
        "    - Performing vector similarity search\n",
        "    - Creating and querying graph structures\n",
        "    - Managing nodes and relationships\n",
        "    - Extracting subgraphs and context\n",
        "    \"\"\"\n",
        "    def __init__(self, uri: str, auth: tuple, database: str = \"memgraph\"):\n",
        "        self.driver = GraphDatabase.driver(uri, auth=auth)\n",
        "        self.database = database\n",
        "        self.driver.verify_connectivity()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the driver connection.\"\"\"\n",
        "        self.driver.close()\n",
        "\n",
        "\n",
        "    def find_similar_nodes(self, embedding: List[float], limit: int = MAX_RESULTS,\n",
        "                          min_similarity: float = VECTOR_SIMILARITY_THRESHOLD,\n",
        "                          min_importance: float = MIN_IMPORTANCE_SCORE) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Hybrid search with metadata filtering and type-specific adjustments.\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        CALL vector_search.search(\"{VECTOR_INDEX_NAME}\", $limit, $embedding)\n",
        "        YIELD node, similarity\n",
        "        WITH node, similarity\n",
        "        WHERE\n",
        "            node.type IN $allowed_types\n",
        "            AND similarity > $min_similarity\n",
        "            AND coalesce(node.importance_score, 1.0) > $min_importance\n",
        "        WITH\n",
        "            node,\n",
        "            similarity,\n",
        "            CASE node.type\n",
        "                WHEN 'EXCLUSION' THEN $exclusion_adjustment * similarity\n",
        "                WHEN 'LIMIT' THEN $limit_adjustment * similarity\n",
        "                ELSE similarity\n",
        "            END as adjusted_score\n",
        "        RETURN\n",
        "            ID(node) as id,\n",
        "            node.text as text,\n",
        "            node.type as type,\n",
        "            node.description as description,\n",
        "            node.value as value,\n",
        "            node.applies_to as applies_to,\n",
        "            node.context as context,\n",
        "            similarity,\n",
        "            node.community as community,\n",
        "            node.betweenness_centrality as centrality\n",
        "        ORDER BY\n",
        "            adjusted_score DESC\n",
        "        LIMIT $limit\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self.driver.execute_query(\n",
        "                query,\n",
        "                embedding=embedding,\n",
        "                limit=limit,\n",
        "                allowed_types=POSSIBLE_ENTITIES,\n",
        "                min_similarity=min_similarity,\n",
        "                min_importance=min_importance,\n",
        "                exclusion_adjustment=SIMILARITY_ADJUSTMENTS[\"EXCLUSION\"],\n",
        "                limit_adjustment=SIMILARITY_ADJUSTMENTS[\"LIMIT\"]\n",
        "            )\n",
        "\n",
        "            # Properly extract records from Neo4j result\n",
        "            records = result.records\n",
        "            print(f\"Found {len(records)} potential nodes\")\n",
        "            if records:\n",
        "                print(f\"First 3 results: {records[:3]}\")\n",
        "\n",
        "            # Convert Neo4j records to dictionaries\n",
        "            return [dict(record.items()) for record in records]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Vector search failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_subgraph_for_nodes(self, nodes: List[Dict], max_depth: int = MAX_HOP_DEPTH) -> Dict:\n",
        "        \"\"\"Extract a subgraph containing the given nodes and their relationships.\"\"\"\n",
        "        if not nodes:\n",
        "            return {\"graph_text\": []}\n",
        "\n",
        "        node_ids = [node[\"id\"] for node in nodes]\n",
        "\n",
        "        query = f\"\"\"\n",
        "            UNWIND $node_ids AS node_id\n",
        "            MATCH (n) WHERE id(n) = node_id\n",
        "            MATCH path = (n)-[r*1..{max_depth}]-(m)\n",
        "            WHERE id(m) <> id(n)\n",
        "            AND NONE(rel IN r WHERE type(rel) IN {IGNORE_RELATIONSHIP_TYPES})\n",
        "\n",
        "            WITH COLLECT(DISTINCT n) + COLLECT(DISTINCT m) AS all_nodes,\n",
        "                COLLECT(DISTINCT path) AS all_paths\n",
        "\n",
        "            UNWIND all_nodes AS node\n",
        "            WITH node, all_paths\n",
        "            WHERE\n",
        "                COALESCE(node.description, \"\") <> \"\" OR\n",
        "                COALESCE(node.text, \"\") <> \"\" OR\n",
        "                COALESCE(TOSTRING(node.value), \"N/A\") <> \"N/A\"\n",
        "\n",
        "            WITH\n",
        "                COLLECT(DISTINCT\n",
        "                    node.type +\n",
        "                    (CASE WHEN COALESCE(node.description, \"\") <> \"\" THEN \": '\" + node.description + \"'\" ELSE \"\" END) +\n",
        "                    (CASE WHEN COALESCE(node.text, \"\") <> \"\" THEN \" - '\" + node.text + \"'\" ELSE \"\" END) +\n",
        "                    (CASE WHEN COALESCE(TOSTRING(node.value), \"N/A\") <> \"N/A\" THEN \" - Value: \" + TOSTRING(node.value) ELSE \"\" END)\n",
        "                ) AS node_descriptions,\n",
        "                all_paths\n",
        "\n",
        "            UNWIND all_paths AS path\n",
        "            UNWIND relationships(path) AS rel\n",
        "            WITH node_descriptions, path,\n",
        "                startNode(rel) AS start_node,\n",
        "                endNode(rel) AS end_node,\n",
        "                type(rel) AS rel_type\n",
        "\n",
        "            WITH node_descriptions, end_node, rel_type, start_node\n",
        "            WHERE\n",
        "                COALESCE(start_node.description, \"\") <> \"\" OR\n",
        "                COALESCE(start_node.text, \"\") <> \"\" OR\n",
        "                COALESCE(TOSTRING(start_node.value), \"N/A\") <> \"N/A\"\n",
        "\n",
        "            WITH node_descriptions, end_node, rel_type,\n",
        "                COLLECT(DISTINCT\n",
        "                    (CASE WHEN COALESCE(start_node.description, \"\") <> \"\" THEN \"'\" + start_node.description + \"'\" ELSE \"\" END) +\n",
        "                    (CASE WHEN COALESCE(start_node.text, \"\") <> \"\" THEN \" - '\" + start_node.text + \"'\" ELSE \"\" END) +\n",
        "                    (CASE WHEN COALESCE(TOSTRING(start_node.value), \"N/A\") <> \"N/A\" THEN \" - Value: \" + TOSTRING(start_node.value) ELSE \"\" END)\n",
        "                ) AS start_node_texts\n",
        "\n",
        "            WITH node_descriptions, end_node, rel_type, start_node_texts,\n",
        "                CASE\n",
        "                    WHEN SIZE(start_node_texts) > 1 THEN\n",
        "                        \"(\" + REDUCE(merged = \"\", text IN start_node_texts |\n",
        "                            CASE WHEN merged = \"\" THEN text ELSE merged + \" OR \" + text END) + \")\"\n",
        "                    ELSE\n",
        "                        HEAD(start_node_texts)\n",
        "                END AS merged_start_nodes\n",
        "\n",
        "            WITH node_descriptions,\n",
        "                merged_start_nodes + \" -[\" + rel_type + \"]-> \" +\n",
        "                \"(\" + end_node.type +\n",
        "                (CASE WHEN COALESCE(end_node.description, \"\") <> \"\" THEN \": '\" + end_node.description + \"'\" ELSE \"\" END) +\n",
        "                (CASE WHEN COALESCE(end_node.text, \"\") <> \"\" THEN \" - '\" + end_node.text + \"'\" ELSE \"\" END) +\n",
        "                (CASE WHEN COALESCE(TOSTRING(end_node.value), \"N/A\") <> \"N/A\" THEN \" - Value: \" + TOSTRING(end_node.value) ELSE \"\" END) +\n",
        "                \")\"\n",
        "                AS grouped_path\n",
        "\n",
        "            WITH node_descriptions, COLLECT(DISTINCT grouped_path) AS optimized_paths\n",
        "            RETURN optimized_paths AS graph_text\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self.driver.execute_query(query, {\"node_ids\": node_ids})\n",
        "            records = result.records\n",
        "\n",
        "            # Simply return the graph_text from the first record\n",
        "            if records and hasattr(records[0], 'get'):\n",
        "                return {\"graph_text\": records[0].get('graph_text', [])}\n",
        "\n",
        "            return {\"graph_text\": []}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting subgraph: {e}\")\n",
        "            return {\"graph_text\": []}\n",
        "\n",
        "\n",
        "    def get_connected_nodes(self, node_ids: List[int]) -> List[str]:\n",
        "        \"\"\"Updated to match project's graph traversal implementation\"\"\"\n",
        "        query = \"\"\"\n",
        "        MATCH (start)-[r*1..5]-(end)\n",
        "        WHERE ID(start) IN $node_ids\n",
        "            AND NONE(rel IN r WHERE type(rel) IN $ignore_rels)\n",
        "        RETURN nodes(r) as path_nodes, relationships(r) as path_rels\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.execute_query(query, {\n",
        "                \"node_ids\": node_ids,\n",
        "                \"ignore_rels\": IGNORE_RELATIONSHIP_TYPES\n",
        "            })\n",
        "            return {\"graph_text\": self._process_paths(result)}\n",
        "        except Exception as e:\n",
        "            print(f\"Graph traversal failed: {str(e)}\")\n",
        "            return {\"graph_text\": []}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "eF1QiXOaNnUG"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# OpenAI Service\n",
        "# =====================================\n",
        "class OpenAIService:\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.embedding_model = \"text-embedding-3-small\"\n",
        "        self.chat_model = \"gpt-4o\"\n",
        "\n",
        "    def get_embedding(self, text: str) -> List[float]:\n",
        "        \"\"\"Get embedding for a text using OpenAI's API.\"\"\"\n",
        "        try:\n",
        "            response = self.client.embeddings.create(\n",
        "                model=self.embedding_model,\n",
        "                input=text\n",
        "            )\n",
        "            return response.data[0].embedding\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting embedding: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def chat_completion(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"Get chat completion from OpenAI's API with JSON enforcement.\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.chat_model,\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                response_format={\"type\": \"json_object\"}  # Enforce JSON output\n",
        "            )\n",
        "            content = response.choices[0].message.content\n",
        "\n",
        "            # Validate JSON structure\n",
        "            json.loads(content)\n",
        "            return content\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Invalid JSON response: {content}\")\n",
        "            raise ValueError(\"LLM returned invalid JSON\") from e\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chat completion: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "z4BPJ4GYNnUG"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Policy Retriever\n",
        "# =====================================\n",
        "class PolicyRetriever:\n",
        "    def __init__(self, memgraph_client: MemgraphClient, openai_service: OpenAIService):\n",
        "        self.mg_client = memgraph_client\n",
        "        self.openai = openai_service\n",
        "        self.system_prompt = \"\"\"**Insurance Policy Analysis Protocol**\n",
        "\n",
        "        1. **Claim-Policy Mapping**\n",
        "        - Match each claim element to EXACT policy clauses\n",
        "        - Require verbatim text matches for conditions\n",
        "\n",
        "        2. **Exclusion Check**\n",
        "        - Check ALL exclusions in context\n",
        "        - Apply exclusion if ANY match\n",
        "\n",
        "        3. **Limit Application**\n",
        "        - Apply MOST SPECIFIC limit first\n",
        "        - Sum applicable limits\n",
        "\n",
        "        4. **Coverage Requirements**\n",
        "        - Verify ALL required conditions met\n",
        "        - Reject if ANY missing\n",
        "\n",
        "        5. **Decision Framework**\n",
        "        IF ANY exclusion applies → Deny\n",
        "        ELIF any condition unmet → Deny\n",
        "        ELIF claim > limit → Partial coverage\n",
        "        ELSE → Full coverage\n",
        "\n",
        "        **Output Requirements**\n",
        "        - Cite EXACT policy text for decisions\n",
        "        - List MISSING requirements if denied\n",
        "        - Calculate SPECIFIC amounts if limited\"\"\"\n",
        "\n",
        "    def _build_prompt(self, claim_text: str, context: str) -> str:\n",
        "        \"\"\"Build the prompt with explicit format requirements\"\"\"\n",
        "        format_example = json.dumps({\n",
        "            \"coverage\": True,\n",
        "            \"explanation\": \"Detailed explanation...\",\n",
        "            \"limits\": [{\n",
        "                \"type\": \"Limit Type\",\n",
        "                \"amount\": 5000,\n",
        "                \"unit\": \"USD\",\n",
        "                \"applies_to\": \"Damage Type\",\n",
        "                \"source\": \"CLAUSE 2.1\"\n",
        "            }],\n",
        "            \"exclusions\": [{\n",
        "                \"text\": \"Exclusion text\",\n",
        "                \"applies\": True,\n",
        "                \"reason\": \"Why it applies\",\n",
        "                \"source\": \"EXCLUSION 3\"\n",
        "            }],\n",
        "            \"description\": \"Brief claim summary\"\n",
        "        }, indent=2)\n",
        "\n",
        "        return f\"\"\"You are an insurance policy analyzer. Analyze this claim:\n",
        "\n",
        "        CLAIM:\n",
        "        {claim_text}\n",
        "\n",
        "        POLICY CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Provide response in EXACTLY this JSON format:\n",
        "        {format_example}\n",
        "\n",
        "        Notes:\n",
        "        - Verify ALL required conditions are met\n",
        "        - Check for ANY applicable exclusions\n",
        "        - Calculate amounts using policy schedules\"\"\"\n",
        "\n",
        "    def build_context_text(self, similar_nodes: List[Dict], connected_data: Dict) -> str:\n",
        "        \"\"\"Policy context with structured node properties and relationships\"\"\"\n",
        "        if not similar_nodes:\n",
        "            return \"No relevant policy information found\"\n",
        "\n",
        "        core_elements = [\n",
        "            f\"({node['type']}) {node.get('text', '')}\"\n",
        "            + (f\"\\n- Value: {node['value']}\" if node.get('value') else \"\")\n",
        "            + (f\"\\n- Description: {node['description']}\" if node.get('description') else \"\")\n",
        "            for node in similar_nodes\n",
        "        ]\n",
        "\n",
        "        return \"\\n\".join([\n",
        "            \"## CORE POLICY ELEMENTS:\",\n",
        "            \"\\n\".join(core_elements),\n",
        "            \"\\n## POLICY RELATIONSHIPS:\",\n",
        "            \"\\n\".join(connected_data.get(\"graph_text\", []))\n",
        "        ])\n",
        "\n",
        "    def reason_about_claim(self, query: str, top_k: int = MAX_RESULTS) -> Dict[str, Any]:\n",
        "        \"\"\"Claim analysis\"\"\"\n",
        "        # Get embedding\n",
        "        query_embedding = self.openai.get_embedding(query)\n",
        "\n",
        "        # Find similar nodes\n",
        "        similar_nodes = self.mg_client.find_similar_nodes(query_embedding, top_k)\n",
        "        if not similar_nodes:\n",
        "            return {\"error\": \"No relevant policy information found\"}\n",
        "\n",
        "        # Extract subgraph\n",
        "        subgraph = self.mg_client.extract_subgraph_for_nodes(similar_nodes)\n",
        "\n",
        "        # Build context\n",
        "        context = self.build_context_text(similar_nodes, subgraph)\n",
        "\n",
        "        print(\"Context:\")\n",
        "        print(context)\n",
        "\n",
        "        # Generate analysis\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": self._build_prompt(query, context)}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = self.openai.chat_completion(messages)\n",
        "            return json.loads(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Analysis failed: {str(e)}\")\n",
        "            return {\"error\": \"Failed to analyze claim\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceGJF47oNnUG",
        "outputId": "7f43d97f-9a90-4406-e09b-56a3d3768d71"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Main Execution\n",
        "# =====================================\n",
        "def main():\n",
        "    # Initialize services\n",
        "    memgraph_client = MemgraphClient(\n",
        "        uri=MEMGRAPH_HOST,\n",
        "        auth=(MEMGRAPH_USER, MEMGRAPH_PASSWORD)\n",
        "    )\n",
        "    openai_service = OpenAIService(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = PolicyRetriever(memgraph_client, openai_service)\n",
        "\n",
        "    # Example query\n",
        "    example_query = \"\"\"\n",
        "    I was driving with 7 passengers from Sheffield to London.\n",
        "    The car broke down on the way and we had to take a hotel room for the night\n",
        "    while the car was being repaired. The invoice is 400 pounds.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Process query\n",
        "        result = retriever.reason_about_claim(example_query, top_k=5)\n",
        "        print(\"\\nResult:\")\n",
        "        print(result)\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        memgraph_client.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
